# PROJECT INTENT & CONTEXT

**Last Updated**: November 11, 2025  
**Project**: Portana - Living AI Personal Assistant

---

## ðŸŽ¯ PROJECT INTENT

Build a **living AI personal assistant** of Aahil Khan that:
- Has comprehensive context about everything professionally: GitHub, LinkedIn, Medium, resume, projects, posts
- Answers ANY question about Aahil with accurate, sourced information
- Provides clickable links to sources when answering
- Serves as a long-lived, evolving digital twin

**Not an MVP.** A fully realized, production-quality product that gets deployed and lives on the internet.

---

## ðŸ“Š CURRENT STATE

### âœ… What Exists
- **Frontend**: Template built, needs refinement
- **Backend**: Template built, needs refinement + data integration
- **Resume Parser**: Implemented (GPT-3.5T, no hallucination)
- **Vector Embeddings**: Qdrant integration ready
- **Chat Service**: Architecture in place (streaming, context-aware)
- **Onboarding Flow**: Steps 1-2 partially working

### âŒ What's Missing
- **Data Ingestion**: GitHub READMEs, Medium articles, projects list not being chunked/embedded
- **Database Persistence**: SQLite schema not implemented
- **Data Flow**: Step 2 â†’ vectors not fully wired
- **Source Linking**: Chat responses don't reference source URLs
- **Frontend Chat**: UI for visitors to ask questions

### ðŸ”§ What Needs Refinement
- UI polish (template â†’ production)
- Backend robustness (error handling, logging)
- Data quality (chunk size, metadata)
- Response accuracy (system prompts, context building)

---

## ðŸ“¥ DATA INGESTION STRATEGY

Manually pump all your data into the system:

### Sources (Priority Order)
1. **Resume** â†’ Parsed into skills, experience, education vectors
2. **GitHub READMEs** â†’ Chunked from all your repos â†’ vectors with repo links
3. **Medium Articles** â†’ Scraped â†’ chunked â†’ vectors with article links
4. **Projects List** â†’ Curated list â†’ vectors with project URLs
5. **LinkedIn** â†’ Profile content â†’ vectors (optional, lower priority)

### Process
```
Source â†’ Extract â†’ Chunk (500 char) â†’ Embed â†’ Qdrant + SQLite
                                           â†“
                          Add metadata: { link, source, projectId, section }
```

### Output
Each vector carries metadata so chat can say:
"You built X using React [github-link]. You also wrote about it here [medium-link]"

---

## ðŸ—£ï¸ CHAT CAPABILITIES (Acceptance Criteria)

The AI **must** be able to:

### âœ… User Profile Questions
- "Who is Aahil Khan?"
- "What's your tech stack?"
- "What's your experience?"
- â†’ Returns: Bio, skills list with categories, experience descriptions
- â†’ Links: LinkedIn, GitHub profile

### âœ… Resume Questions
- "What skills do you have?"
- "Tell me about your experience"
- "What's your education?"
- â†’ Returns: Resume data accurately
- â†’ Links: Resume download (if applicable)

### âœ… Project Questions
- "What projects have you built?"
- "Show me your latest work"
- "What did you build with React?"
- â†’ Returns: Project name, description, tech stack, key features
- â†’ Links: GitHub repo, deployed URL, Medium article about it

### âœ… Technical Questions
- "What's your approach to system design?"
- "How do you handle authentication?"
- "What practices do you follow in backend?"
- â†’ Returns: Answer grounded in your projects + resume
- â†’ Links: Specific GitHub repos, Medium articles explaining approach

### âœ… Content Questions
- "Summarize your latest Medium posts"
- "What have you written about recently?"
- â†’ Returns: Article summaries, key takeaways
- â†’ Links: Medium article URLs

### Acceptance Point
System is "done" when all above categories work with accurate responses and proper source attribution.

---

## ðŸ—ï¸ TECHNICAL ARCHITECTURE

### Data Layer
- **Vector DB**: Qdrant (semantic search)
- **Relational DB**: SQLite (persistence, metadata)
- **Content**: Chunks with { text, source, link, projectId, section }

### Processing Layer
- **Resume Parser**: GPT-3.5T (skills, experience, education)
- **Embedder**: OpenAI text-embedding-3-small (1536 dims)
- **Chunker**: Split content ~500 chars, preserve metadata
- **Deduplicator**: SHA256 hashing to avoid duplicate vectors

### Retrieval Layer
- **Retriever**: Vector search in Qdrant, top-3 results with scores
- **Reranker**: Diversify by source (skill, experience, article, etc)
- **Context Builder**: Assemble system prompt + retrieved chunks

### Chat Layer
- **ChatService**: Streaming + non-streaming responses
- **System Prompt**: Instructs AI to reference sources + include links
- **Memory**: Session-based conversation history
- **Output**: Text with embedded source links

### Admin Layer
- **Ingestors**: GitHub (fetch READMEs), Medium (fetch articles), Projects (manual)
- **Endpoints**: /api/admin/ingest/*, /api/admin/sources, /api/admin/status
- **Debugging**: /api/onboarding/*/debug, /api/admin/logs

---

## ðŸ”„ DATA FLOW (Simplified)

```
Manual Ingestion:
  Resume (Step 2) â†’ Parse â†’ Embed â†’ Qdrant
  GitHub READMEs â†’ Chunk â†’ Embed â†’ Qdrant
  Medium Articles â†’ Chunk â†’ Embed â†’ Qdrant
  Projects List â†’ Chunk â†’ Embed â†’ Qdrant
                         â†“
                   SQLite Backup

Visitor asks question:
  Question â†’ Embed â†’ Search Qdrant (top-3)
                        â†“
                   Build Context
                   (retrieved chunks + metadata)
                        â†“
                   Build System Prompt
                   "Answer based on: [context]
                    Reference sources when mentioning"
                        â†“
                   Call GPT-4 / GPT-3.5T
                   (streaming)
                        â†“
                   Response with links
```

---

## ðŸŽ¯ MVP SUCCESS CRITERIA (Not an MVP, but checkpoint)

When complete, you should be able to:

1. âœ… Visit portana.aahil-khan.tech
2. âœ… Ask "Who are you?"
3. âœ… Get: "I'm Aahil Khan, 3rd year CS student..." with links to LinkedIn
4. âœ… Ask "What projects have you built?"
5. âœ… Get: Accurate list with GitHub + Medium links
6. âœ… Ask "Explain your backend architecture"
7. âœ… Get: Answer grounded in your experience + projects with source links
8. âœ… Ask "Summarize your latest content"
9. âœ… Get: Recent posts/projects with content summaries

---

## ðŸ“… PHASES (For Reference)

### Phase 1: Data Ingestion (This Week)
- [ ] Create projects.json with all your projects
- [ ] Build GitHub README ingestor
- [ ] Build Medium article ingestor
- [ ] Manually run ingestors to populate Qdrant + SQLite
- [ ] Verify all data is searchable

### Phase 2: Chat Refinement (Next)
- [ ] Build system prompts that reference sources
- [ ] Ensure responses include clickable links
- [ ] Test accuracy of answers
- [ ] Handle edge cases (unknown topics)

### Phase 3: Frontend (After)
- [ ] Build chat UI for visitors
- [ ] Test UX + streaming display
- [ ] Polish design (template â†’ production)

### Phase 4: Deployment (Final)
- [ ] Deploy backend with embedded data
- [ ] Deploy frontend
- [ ] Monitor + iterate

---

## ðŸ”— KEY DOCUMENTS

Reference these as we move forward:
- `BACKEND_ARCHITECTURE.md` - System design + data flow
- `DATA_INGESTION_STRATEGY.md` - How to ingest GitHub/Medium/Projects
- `ONBOARDING_FLOW_ANALYSIS.md` - Step 1-2 current state

---

## ðŸ’¬ DISCUSSION FRAMEWORK

Before discussing architecture changes:
1. **Verify alignment**: Does this align with making AI answer ANY question about Aahil?
2. **Check acceptance**: Does this move toward the success criteria?
3. **Consider scope**: Is this a refinement or scope creep?
4. **Evaluate tradeoff**: Time investment vs. quality improvement?

---

## âš ï¸ NON-GOALS

- Don't build multi-user onboarding (one host = one user)
- Don't optimize for self-service (this is custom for Aahil)
- Don't build public AI marketplace (personal assistant only)
- Don't delay for "perfect" UI (function first, polish second)
- Don't settle for hallucination (accuracy > speed)

---

## ðŸš€ STARTING POINT

**Today's focus**: Get data ingestion working so chat can reference everything.

1. Create `projects.json` with your projects
2. Build GitHub ingestor (fetch + chunk READMEs)
3. Build Medium ingestor (fetch + chunk articles)
4. Run ingestion â†’ verify Qdrant populated
5. Test chat responds with links

Once data ingestion works â†’ chat refinement becomes straightforward.
